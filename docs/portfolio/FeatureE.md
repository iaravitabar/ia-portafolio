---
title: "Featrure Engineering Simple + Modelo Base ‚öôÔ∏è"
date: 12-08-2025
---

# Feature Engineering Simple + Modelo Base ‚öôÔ∏è

Para esta segunda pr√°ctica retomamos la pr√°ctica anterior sobre el dataset del Titanic para documentar el proceso de caracteristicas simple y la creacion del modelo de lasificacion base para el dataset. 

## Contexto ü§îüí≠

El objetivo de esta pr√°ctica es tomar un conjunto de datos (el del Titanic), y aplicar t√©cnicas b√°sicas de Feature Engineering para mejorar la calidad y las predicciones, para entrenar un modelo simple de Regresi√≥n Log√≠stica. Fundamentalmente, se busca establecer un modelo base (baseline) para tener una referencia clara del rendimiento y asegurar que nuestro modelo es √∫til.

### Actividad 0: Investigaci√≥n Te√≥rica
Antes de seguir con la implementaci√≥n, me gustar√≠a dejar registrada la actividad 0 como huella te√≥rica. Me sirvi√≥ para entender componentes clave que se usar√°n. Las preguntas gu√≠a fueron:

??? tip "Investigaci√≥n de Scikit-learn"
    ###`LogisticRegression`:
    - **¬øQu√© tipo de problemas resuelve?**
        Resuelve problemas de clasificaci√≥n.  Estima prob de que una instancia sea de una clase espec√≠fica. Mapea la salida en prob entre 1 y 0
    - **`C`**
        par√°metro de regularzaci√≥n. Inverso a fuerza refularizaci√≥n, valores m√°s peque√±os indican una regularizaci√≥n m√°s fuerte
    - **`penalty`:** 
        especifica la norma de regularizaci√≥n a usar ('l2'mas comun) mientras que 'l1' puede llevar a la selecci√≥n de caracter√≠sticas (algunos coeficientes se vuelven cero)
    - **`solver`:** 
        algoritmo a usar en el problema de optimizaci√≥n. La elecci√≥n depende del tama√±o del dataset y la penalizaci√≥n elegida
    - **`liblinear`** 
        es una buena opci√≥n para **datasets peque√±os**. Funciona bien con penalizaciones L1 y L2
    - Para **datasets grandes**, solvers como `sag`, `saga`son m√°s r√°pidos. No todos los solvers soportan todas las penalizaciones (`lbfgs` solo soporta 'l2')

    ###`DummyClassifier`:
    - **¬øPara qu√© sirve exactamente?**
      - Sirve para crear un **baseline o pto de referencia**.clasificador muy simple que no aprende de los datos, sino hace predicciones basadas en estrategias triviales
    - **¬øQu√© estrategias de baseline ofrece?**
      -`most_frequent`: siempre predice la clase m√°s com√∫n en el conjunto de entrenamiento
      -`stratified`: hace predicciones aleatorias pero manteniendo la distribuci√≥n de clases del conj de entrenamiento
      -`uniform`: predice clases de forma aleatoria con = probabilidad para cada una
    - **¬øPor qu√© es importante tener un baseline?**
      -importancia baseline: **contextualizar el rendimiento** del modelo. Si un modelo complejo no supera significativamente a un baseline simple, significa que el modelo no est√°             aprendiendo patrones √∫tiles o que el problema es muy dif√≠cil. Nos dice si nuestro esfuerzo est√° valiendo la pena
          
    ###`train_test_split`:
    - **¬øQu√© hace `stratify`?**
      - hace que la divisi√≥n de los datos mantenga la **misma proporci√≥n de cada clase** que en el conjunto de datos original. Es fundamental en problemas con clases                            desbalanceadas para que tanto el conjunto de entrenamiento como el de prueba sean representativos
    - **Por qu√© usar `random_state`?**
      - para garantizar la **reproducibilidad**. Al fijar un `random_state` (cualquier num entero), la div de los datos ser√° siempre la misma cada vez que se     ejecute el                     c√≥digo.clave para depurar y comparar el rendimiento de diferentes modelos de manera justa
    - **¬øQu√© porcentaje de test es recomendable?**
      - No hay regla √∫nica, pero las divisiones comunes son **80/20** o **70/30** (entrenamiento/prueba). Para datasets muy grandes, el porcentaje de prueba puede ser menor. Un                 20% es un buen punto de partida 
        
    ###`m√©tricas de evaluaci√≥n`:
    - **¬øQu√© significa cada m√©trica en `classification_report`?**
      - `precision`: De todas las veces que el modelo predijo una clase, ¬øQUE porcentaje fue correcto? (Mide calidad de las predicciones positivas)
      - `recall` (sensibilidad): De todos los ejemplos reales de una clase, ¬øqu√© porcentaje detect√≥ el modelo? (Mide capacidad del modelo para encontrar todos los pos)
      - `f1-score`: Es la media de `precision` y `recall`. Es una m√©trica √∫nica que balancea las dos, √∫til cuando nos importan tanto los falsos positivos como los falsos negativos
      - `support`: n√∫mero de muestras reales de cada clase en los datos

    - **¬øC√≥mo interpretar la matriz de confusi√≥n?**
      -Es una tabla que resume los aciertos y errores. Para una clasificaci√≥n binaria:
      - **Diagonal principal (arriba-izq a abajo-der)**: Los aciertos (Verdaderos Nega, Verdaderos Posi)
      - **Fuera de la diagonal**: Los errores (Falsos Posi, Falsos Nega)

    - **¬øCu√°ndo usar `accuracy` vs otras m√©tricas?**
      - `accuracy` (precisi√≥n global) cuando las **clases est√°n balanceadas**
      - `precision`, `recall` y `f1-score` cuando las **clases est√°n desbalanceadas**. `accuracy` puede ser tricky, xq un modelo puede tener una alta precisi√≥n solo                              prediciendo siempre la clase mayoritaria


## Objetivos 

## Actividades

## Desarrollo

## Evidencias
- resulta2

## Reflexiones

## Ref
